"""
AURA AI Core Microservice
Retinal Image Analysis Service using Hugging Face OCT Classifier
Model: tomalmog/oct-retinal-classifier (EfficientNet-B3, 99.6% accuracy)
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, HttpUrl, Field
from typing import Optional, Dict, Any, List, Tuple
import uvicorn
import logging
from datetime import datetime
import os
import uuid
from pathlib import Path

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="AURA AI Core Microservice",
    description="""
    ## Comprehensive AI Understanding Retinal Analysis (AURA)
    
    **Using retinal imaging as a "window" to systemic health**
    
    ### üî¨ Model Information
    - **Model**: tomalmog/oct-retinal-classifier
    - **Architecture**: EfficientNet-B3
    - **Accuracy**: 99.6% on test set
    - **Source**: [Hugging Face](https://huggingface.co/tomalmog/oct-retinal-classifier)
    
    ### üëÅÔ∏è Eye Conditions Detected
    - **CNV**: Choroidal Neovascularization
    - **DME**: Diabetic Macular Edema
    - **DRUSEN**: Early AMD (Age-related Macular Degeneration)
    - **NORMAL**: Healthy Retina
    
    ### ‚ù§Ô∏è Systemic Health Risk Assessment (AURA Core Feature)
    Based on retinal vascular analysis, AURA estimates risks for:
    - **Cardiovascular Disease**: Heart disease risk from vascular changes
    - **Diabetes Complications**: Risk based on diabetic retinopathy signs
    - **Hypertension**: Blood pressure risk from vessel abnormalities
    - **Stroke**: Cerebrovascular risk assessment
    
    ### üìã Clinical Decision Support
    AURA acts as a Clinical Decision Support (CDS) tool that assists ‚Äî not replaces ‚Äî physicians.
    """,
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify allowed origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# Static files for explainability images (heatmaps & annotated images)
# ============================================================================
BASE_DIR = Path(__file__).resolve().parent.parent
HEATMAPS_DIR = BASE_DIR / "heatmaps"
ANNOTATED_DIR = BASE_DIR / "annotated-images"

# ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i ƒë·ªÉ StaticFiles kh√¥ng b·ªã l·ªói khi kh·ªüi ƒë·ªông
HEATMAPS_DIR.mkdir(parents=True, exist_ok=True)
ANNOTATED_DIR.mkdir(parents=True, exist_ok=True)

# Serve c√°c file PNG gi·∫£i th√≠ch ƒë·ªÉ frontend truy c·∫≠p tr·ª±c ti·∫øp
app.mount(
    "/heatmaps",
    StaticFiles(directory=str(HEATMAPS_DIR)),
    name="heatmaps",
)
app.mount(
    "/annotated-images",
    StaticFiles(directory=str(ANNOTATED_DIR)),
    name="annotated-images",
)

# ============================================================================
# Request/Response Models
# ============================================================================

class AnalyzeRequest(BaseModel):
    """Request model for image analysis"""
    image_url: HttpUrl
    image_type: str = Field(default="OCT", description="Image type: OCT or Fundus")
    model_version: Optional[str] = None

class ConditionResult(BaseModel):
    """Result for a single condition"""
    probability: float
    positive: bool
    threshold: float
    severity: str
    clinical_name: str
    icd10: Optional[str] = None
    description: str
    confidence_interval: Dict[str, float]

class RiskAssessment(BaseModel):
    """Overall risk assessment"""
    risk_level: str
    risk_score: float
    primary_condition: str
    positive_conditions: List[str]
    urgency: str
    requires_referral: bool

class AnalysisResult(BaseModel):
    """Complete analysis result model for AURA Retinal Screening System"""
    analysis_id: str
    image_url: str
    image_type: str
    
    # Primary prediction
    predicted_class: str
    confidence: float
    
    # Risk assessment
    risk_level: str
    risk_score: float
    
    # Eye condition results
    conditions: Dict[str, Dict[str, Any]]
    risk_assessment: Dict[str, Any]
    findings: List[Dict[str, Any]]
    recommendations: List[str]
    
    # AURA Core Feature: Systemic Health Risk Assessment
    # Using retinal imaging as "window" to cardiovascular, diabetes, hypertension, stroke risks
    systemic_health_risks: Dict[str, Dict[str, Any]] = Field(
        default={},
        description="Systemic health risks derived from retinal findings (cardiovascular, diabetes, hypertension, stroke)"
    )
    
    # Retinal vascular metrics (for 'B·∫•t th∆∞·ªùng M·∫°ch m√°u' section on UI)
    vascular_metrics: Dict[str, Any] = Field(
        default={},
        description="Quantitative metrics describing retinal vasculature (tortuosity, caliber variation, microaneurysms, hemorrhage score)"
    )
    
    # Annotations
    annotations: Optional[Dict[str, Any]] = None
    heatmap_url: Optional[str] = None
    annotated_image_url: Optional[str] = None
    
    # Metadata
    processed_at: datetime
    model_version: str
    model_info: Dict[str, Any]
    processing_time_ms: int
    image_quality: Dict[str, Any]


class BatchAnalyzeRequest(BaseModel):
    """
    Batch analysis request cho AI Core.

    L∆∞u √Ω:
    - `items` l√† danh s√°ch c√°c y√™u c·∫ßu ph√¢n t√≠ch gi·ªëng `AnalyzeRequest`.
    - `batch_id` cho ph√©p backend g·∫Øn k·∫øt v·ªõi job/batch ·ªü ph√≠a .NET.
    """
    items: List[AnalyzeRequest] = Field(
        ..., description="Danh s√°ch ·∫£nh c·∫ßn ph√¢n t√≠ch (t·ªëi thi·ªÉu 1 ·∫£nh, khuy·∫øn ngh·ªã >= 100 cho ch·∫ø ƒë·ªô batch)."
    )
    batch_id: Optional[str] = Field(
        default=None,
        description="M√£ batch do backend t·∫°o (t√πy ch·ªçn). N·∫øu kh√¥ng truy·ªÅn, AI Core s·∫Ω t·ª± sinh."
    )


class BatchAnalysisSummary(BaseModel):
    """T·ªïng quan k·∫øt qu·∫£ batch ƒë·ªÉ backend hi·ªÉn th·ªã nhanh."""
    batch_id: str
    total: int
    success_count: int
    failed_count: int
    processing_time_ms: int


class BatchAnalysisResponse(BaseModel):
    """
    K·∫øt qu·∫£ ph√¢n t√≠ch batch.

    - `results`: danh s√°ch `AnalysisResult` (ch·ªâ ch·ª©a nh·ªØng ·∫£nh x·ª≠ l√Ω th√†nh c√¥ng).
    - `errors`: danh s√°ch l·ªói theo ·∫£nh ƒë·ªÉ backend hi·ªÉn th·ªã cho ng∆∞·ªùi d√πng.
    """
    summary: BatchAnalysisSummary
    results: List[AnalysisResult]
    errors: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="Danh s√°ch l·ªói theo t·ª´ng ·∫£nh: index, image_url, error_message."
    )

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    version: str
    timestamp: datetime
    uptime_seconds: float
    model_loaded: bool
    model_info: Optional[Dict[str, Any]] = None

# ============================================================================
# Global State
# ============================================================================

app_start_time = datetime.now()
model_loaded = False
model_service = None

# ============================================================================
# Health Check Endpoint
# ============================================================================

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    Health check endpoint for Docker and load balancer
    Returns service health status and model information
    """
    uptime = (datetime.now() - app_start_time).total_seconds()
    
    return HealthResponse(
        status="healthy",
        version="2.0.0",
        timestamp=datetime.now(),
        uptime_seconds=uptime,
        model_loaded=model_loaded,
        model_info=model_service.get_model_info() if model_service else None
    )

@app.get("/api/health", response_model=HealthResponse)
async def health_check_api():
    """Health check endpoint at /api/health"""
    return await health_check()

# ============================================================================
# Analysis Endpoint
# ============================================================================

@app.post("/api/analyze", response_model=AnalysisResult)
async def analyze_image(request: AnalyzeRequest, background_tasks: BackgroundTasks):
    """
    Analyze retinal image and return comprehensive risk assessment
    
    Uses the tomalmog/oct-retinal-classifier model for OCT image classification.
    
    **Supported conditions:**
    - CNV (Choroidal Neovascularization)
    - DME (Diabetic Macular Edema)
    - DRUSEN (Early AMD)
    - NORMAL (Healthy Retina)
    
    Args:
        request: AnalyzeRequest with image_url and image_type
        
    Returns:
        AnalysisResult with comprehensive risk assessment and clinical findings
    """
    start_time = datetime.now()
    
    try:
        logger.info(f"Analyzing image: {request.image_url}, Type: {request.image_type}")
        
        result = await perform_analysis(request)
        
        processing_time = int((datetime.now() - start_time).total_seconds() * 1000)
        result.processing_time_ms = processing_time
        
        logger.info(f"Analysis completed in {processing_time}ms - Prediction: {result.predicted_class}")
        
        return result
        
    except Exception as e:
        logger.error(f"Error analyzing image: {str(e)}", exc_info=True)
        # Tr·∫£ v·ªÅ th√¥ng b√°o ti·∫øng Vi·ªát th√¢n thi·ªán, v·∫´n gi·ªØ chi ti·∫øt k·ªπ thu·∫≠t trong log
        raise HTTPException(
            status_code=500,
            detail=f"Kh√¥ng th·ªÉ ph√¢n t√≠ch ·∫£nh v√µng m·∫°c. Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá h·ªó tr·ª£. Chi ti·∫øt k·ªπ thu·∫≠t: {str(e)}"
        )


@app.post("/api/analyze-batch", response_model=BatchAnalysisResponse)
async def analyze_images_batch(request: BatchAnalyzeRequest):
    """
    Ph√¢n t√≠ch nhi·ªÅu ·∫£nh v√µng m·∫°c trong m·ªôt l·∫ßn g·ªçi (batch analysis).

    - Ph√π h·ª£p v·ªõi NFR-2: x·ª≠ l√Ω l√¥ ‚â• 100 ·∫£nh.
    - M·ªói ph·∫ßn t·ª≠ trong `items` t∆∞∆°ng ƒë∆∞∆°ng m·ªôt l·∫ßn g·ªçi `/api/analyze`.
    """
    if not request.items:
        raise HTTPException(
            status_code=400,
            detail="Danh s√°ch ·∫£nh c·∫ßn ph√¢n t√≠ch kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng."
        )

    max_batch_size = int(os.getenv("MAX_BATCH_SIZE", "200"))
    if len(request.items) > max_batch_size:
        raise HTTPException(
            status_code=400,
            detail=f"S·ªë l∆∞·ª£ng ·∫£nh trong m·ªôt l·∫ßn ph√¢n t√≠ch t·ªëi ƒëa l√† {max_batch_size}. "
                   f"Hi·ªán t·∫°i b·∫°n ƒëang g·ª≠i {len(request.items)} ·∫£nh."
        )

    batch_id = request.batch_id or str(uuid.uuid4())
    start_time = datetime.now()

    results: List[AnalysisResult] = []
    errors: List[Dict[str, Any]] = []

    for idx, item in enumerate(request.items):
        try:
            logger.info(f"[Batch {batch_id}] Ph√¢n t√≠ch ·∫£nh {idx + 1}/{len(request.items)}: {item.image_url}")
            result = await analyze_image(item, BackgroundTasks())
            results.append(result)
        except HTTPException as http_ex:
            logger.warning(
                f"[Batch {batch_id}] ·∫¢nh index={idx} l·ªói HTTP {http_ex.status_code}: {http_ex.detail}"
            )
            errors.append({
                "index": idx,
                "image_url": str(item.image_url),
                "status_code": http_ex.status_code,
                "message": http_ex.detail,
            })
        except Exception as ex:
            logger.error(f"[Batch {batch_id}] L·ªói kh√¥ng mong ƒë·ª£i khi ph√¢n t√≠ch ·∫£nh index={idx}: {str(ex)}")
            errors.append({
                "index": idx,
                "image_url": str(item.image_url),
                "status_code": 500,
                "message": f"L·ªói n·ªôi b·ªô khi ph√¢n t√≠ch ·∫£nh: {str(ex)}",
            })

    total = len(request.items)
    success_count = len(results)
    failed_count = len(errors)
    processing_time_ms = int((datetime.now() - start_time).total_seconds() * 1000)

    logger.info(
        f"[Batch {batch_id}] Ho√†n th√†nh ph√¢n t√≠ch batch: t·ªïng={total}, th√†nh c√¥ng={success_count}, "
        f"l·ªói={failed_count}, th·ªùi gian={processing_time_ms}ms"
    )

    summary = BatchAnalysisSummary(
        batch_id=batch_id,
        total=total,
        success_count=success_count,
        failed_count=failed_count,
        processing_time_ms=processing_time_ms,
    )

    return BatchAnalysisResponse(
        summary=summary,
        results=results,
        errors=errors,
    )

async def perform_analysis(request: AnalyzeRequest) -> AnalysisResult:
    """
    Perform complete AI analysis on the image
    
    Pipeline:
    1. Download image from URL
    2. Validate image quality
    3. Preprocess image
    4. Run AI model inference
    5. Generate clinical findings and recommendations
    6. Generate annotations
    """
    import numpy as np
    
    try:
        from app.image_processor import ImageProcessor
    except ImportError:
        from image_processor import ImageProcessor
    
    analysis_id = str(uuid.uuid4())
    processor = ImageProcessor()
    
    try:
        # 1. Download image
        logger.info("Downloading image...")
        image_array, metadata = await processor.download_image(str(request.image_url))
        
        # 2. Validate image quality
        logger.info("Validating image...")
        validation = processor.validate_image(image_array, metadata)
        
        if not validation['is_valid']:
            logger.warning(f"Image validation issues: {validation['issues']}")
        
        # 3. Run AI model inference
        logger.info("Running model inference...")
        if model_service and model_service.model_loaded:
            try:
                # Model handles its own preprocessing
                model_results = model_service.predict(image_array)
                logger.info(f"Model prediction: {model_results.get('predicted_class', 'Unknown')}")
            except Exception as e:
                logger.error(f"Model prediction failed: {str(e)}")
                model_results = generate_fallback_results()
        else:
            logger.warning("Model not loaded, using fallback")
            model_results = generate_fallback_results()
        
        # 4. Extract results
        predicted_class = model_results.get('predicted_class', 'NORMAL')
        confidence = model_results.get('confidence', 0.5)
        conditions = model_results.get('conditions', {})
        risk_assessment = model_results.get('risk_assessment', {})
        recommendations = model_results.get('recommendations', [])
        
        # 5. Extract AURA systemic health risks (cardiovascular, diabetes, hypertension, stroke)
        systemic_health_risks = model_results.get('systemic_health_risks', {})
        
        # 6. Generate findings (b·ªánh t·∫°i m·∫Øt + ch·∫•t l∆∞·ª£ng ·∫£nh)
        findings = generate_findings_from_results(model_results, validation)
        
        # 7. Generate annotations, vascular metrics & ƒëi·ªÅu ch·ªânh nguy c∆° to√†n th√¢n
        processed_for_features = processor.preprocess_image(image_array)
        features = processor.analyze_image_features(processed_for_features)
        annotations = generate_annotations(features, image_array.shape)
        
        vascular_metrics = features.get('vascular_metrics', {})
        
        # 7a. X√°c ƒë·ªãnh xem c√≥ b·∫•t th∆∞·ªùng (b·ªánh) hay kh√¥ng ƒë·ªÉ quy·∫øt ƒë·ªãnh sinh h√¨nh ·∫£nh gi·∫£i th√≠ch
        # Quan tr·ªçng: C·∫ßn check nhi·ªÅu ƒëi·ªÅu ki·ªán ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªè s√≥t
        has_positive_condition = False
        try:
            # ƒêi·ªÅu ki·ªán 1: predicted_class t·ª´ model kh√¥ng ph·∫£i NORMAL
            if predicted_class and predicted_class != 'NORMAL':
                has_positive_condition = True
                logger.info(f"[EXPLAINABILITY] Ph√°t hi·ªán b·ªánh t·ª´ predicted_class: {predicted_class}")
            
            # ƒêi·ªÅu ki·ªán 2: primary_condition trong risk_assessment
            if not has_positive_condition:
                primary_condition = risk_assessment.get('primary_condition')
                if primary_condition and primary_condition != 'NORMAL':
                    has_positive_condition = True
                    logger.info(f"[EXPLAINABILITY] Ph√°t hi·ªán b·ªánh t·ª´ primary_condition: {primary_condition}")
            
            # ƒêi·ªÅu ki·ªán 3: B·∫•t k·ª≥ condition n√†o c√≥ positive == True
            if not has_positive_condition:
                for cond_name, cond_data in conditions.items():
                    if isinstance(cond_data, dict) and cond_data.get('positive') and cond_name != 'NORMAL':
                        has_positive_condition = True
                        logger.info(f"[EXPLAINABILITY] Ph√°t hi·ªán b·ªánh t·ª´ condition positive: {cond_name}")
                        break
            
            # ƒêi·ªÅu ki·ªán 4: Confidence cao v·ªõi class kh√¥ng ph·∫£i NORMAL
            if not has_positive_condition and confidence and confidence > 0.5 and predicted_class != 'NORMAL':
                has_positive_condition = True
                logger.info(f"[EXPLAINABILITY] Ph√°t hi·ªán b·ªánh t·ª´ confidence > 0.5: {predicted_class} ({confidence:.2%})")
            
            logger.info(f"[EXPLAINABILITY] has_positive_condition = {has_positive_condition}")
        except Exception as e_flag:
            logger.warning(f"Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c tr·∫°ng th√°i b·ªánh d∆∞∆°ng t√≠nh: {str(e_flag)}")
            # Fallback: N·∫øu c√≥ l·ªói nh∆∞ng predicted_class kh√¥ng ph·∫£i NORMAL, v·∫´n sinh ·∫£nh
            if predicted_class and predicted_class != 'NORMAL':
                has_positive_condition = True
        
        # 7b. B·ªï sung c√°c finding li√™n quan m·∫°ch m√°u
        vascular_findings = generate_vascular_findings(vascular_metrics)
        findings.extend(vascular_findings)
        
        # 7c. ƒêi·ªÅu ch·ªânh systemic_health_risks & risk_assessment d·ª±a tr√™n m·∫°ch m√°u
        if systemic_health_risks and vascular_metrics:
            systemic_health_risks, risk_assessment = enrich_systemic_risks_with_vascular(
                systemic_health_risks,
                vascular_metrics,
                risk_assessment
            )
        
        # 8. Log systemic health risks
        if systemic_health_risks:
            high_risks = [k for k, v in systemic_health_risks.items() if v.get('risk_level') in ['High', 'Moderate']]
            if high_risks:
                logger.info(f"Systemic health risks detected: {high_risks}")
        
        # 9. Generate heatmap (n·∫øu model h·ªó tr·ª£) v√† l∆∞u file ƒë·ªÉ frontend hi·ªÉn th·ªã
        heatmap_url: Optional[str] = None
        logger.info(f"[HEATMAP] B·∫Øt ƒë·∫ßu sinh heatmap cho analysis_id={analysis_id}, has_positive_condition={has_positive_condition}")
        
        try:
            import cv2  # type: ignore
            
            heatmap_array = model_results.get("heatmap")
            logger.info(f"[HEATMAP] Model tr·∫£ v·ªÅ heatmap: {heatmap_array is not None}")

            # N·∫øu m√¥ h√¨nh kh√¥ng tr·∫£ v·ªÅ heatmap nh∆∞ng c√≥ b·ªánh, t·∫°o heatmap ƒë∆°n gi·∫£n t·ª´ ·∫£nh ƒë√£ ti·ªÅn x·ª≠ l√Ω
            if heatmap_array is None and has_positive_condition:
                logger.info("[HEATMAP] T·∫°o heatmap fallback t·ª´ ·∫£nh g·ªëc...")
                if image_array.ndim == 3:
                    # D√πng k√™nh luma ƒë∆°n gi·∫£n l√†m heatmap
                    gray = np.mean(image_array.astype("float32"), axis=2)
                else:
                    gray = image_array.astype("float32")
                # Chu·∫©n ho√° 0-1
                gmin, gmax = gray.min(), gray.max()
                if gmax > gmin:
                    heatmap_array = (gray - gmin) / (gmax - gmin)
                else:
                    heatmap_array = np.zeros_like(gray, dtype="float32")
                logger.info(f"[HEATMAP] ƒê√£ t·∫°o heatmap fallback, shape={heatmap_array.shape}")

            if heatmap_array is not None:
                # L∆∞u heatmap ra th∆∞ m·ª•c ./heatmaps d∆∞·ªõi d·∫°ng PNG
                heatmaps_dir = Path(__file__).resolve().parent.parent / "heatmaps"
                heatmaps_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"[HEATMAP] Th∆∞ m·ª•c heatmaps: {heatmaps_dir}, exists={heatmaps_dir.exists()}")

                heatmap_path = heatmaps_dir / f"{analysis_id}.png"

                # ƒê·∫£m b·∫£o l√† ·∫£nh 2D ho·∫∑c 3D v√† ·ªü d·∫°ng uint8
                if heatmap_array.ndim == 2:
                    norm = (heatmap_array - heatmap_array.min()) / (
                        (heatmap_array.max() - heatmap_array.min()) or 1.0
                    )
                    heatmap_uint8 = (norm * 255).astype("uint8")
                    heatmap_bgr = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
                else:
                    heatmap_bgr = heatmap_array

                success = cv2.imwrite(str(heatmap_path), heatmap_bgr)
                logger.info(f"[HEATMAP] cv2.imwrite success={success}, path={heatmap_path}")

                if success:
                    # URL t∆∞∆°ng ƒë·ªëi ƒë·ªÉ backend/nginx c√≥ th·ªÉ expose (v√≠ d·ª• mount th∆∞ m·ª•c n√†y)
                    heatmap_url = f"/heatmaps/{analysis_id}.png"
                    logger.info(f"[HEATMAP] ‚úÖ ƒê√£ sinh heatmap th√†nh c√¥ng: {heatmap_url}")
                else:
                    logger.error(f"[HEATMAP] ‚ùå cv2.imwrite th·∫•t b·∫°i cho {heatmap_path}")
            else:
                logger.info("[HEATMAP] Kh√¥ng c√≥ heatmap ƒë·ªÉ l∆∞u (heatmap_array is None)")
        except Exception as e:
            logger.error(f"[HEATMAP] ‚ùå Exception khi sinh heatmap cho analysis_id={analysis_id}: {str(e)}", exc_info=True)
            heatmap_url = None

        # 9b. Generate annotated image (·∫£nh g·ªëc + v√πng b·∫•t th∆∞·ªùng) ƒë·ªÉ ph·ª•c v·ª• FR-4
        annotated_image_url: Optional[str] = None
        logger.info(f"[ANNOTATED] B·∫Øt ƒë·∫ßu sinh annotated image cho analysis_id={analysis_id}")
        
        try:
            import cv2  # type: ignore
            
            # Ch·ªâ t·∫°o annotated image n·∫øu c√≥ annotations v·ªÅ v√πng b·∫•t th∆∞·ªùng
            regions: List[Dict[str, Any]] = []
            if annotations and isinstance(annotations, dict):
                regions = list(annotations.get("abnormal_regions") or [])
            logger.info(f"[ANNOTATED] S·ªë regions t·ª´ annotations: {len(regions)}")

            # N·∫øu c√≥ b·ªánh nh∆∞ng ch∆∞a c√≥ v√πng b·∫•t th∆∞·ªùng, t·∫°o m·ªôt v√πng m·∫∑c ƒë·ªãnh ·ªü g·∫ßn trung t√¢m ·∫£nh
            if has_positive_condition and not regions:
                logger.info("[ANNOTATED] T·∫°o region fallback ·ªü gi·ªØa ·∫£nh...")
                h, w = image_array.shape[:2]
                box_w = max(40, w // 4)
                box_h = max(40, h // 4)
                x = max(0, (w - box_w) // 2)
                y = max(0, (h - box_h) // 2)
                regions.append({
                    "x": int(x),
                    "y": int(y),
                    "width": int(box_w),
                    "height": int(box_h),
                    "type": "suspected_region",
                    "confidence": float(risk_assessment.get("combined_risk_score", risk_assessment.get("risk_score", 0.5)))
                })
                if not annotations or not isinstance(annotations, dict):
                    annotations = {}
                annotations["abnormal_regions"] = regions
                logger.info(f"[ANNOTATED] ƒê√£ t·∫°o region fallback: x={x}, y={y}, w={box_w}, h={box_h}")

            if regions:
                annotated_dir = Path(__file__).resolve().parent.parent / "annotated-images"
                annotated_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"[ANNOTATED] Th∆∞ m·ª•c annotated-images: {annotated_dir}, exists={annotated_dir.exists()}")

                annotated_path = annotated_dir / f"{analysis_id}.png"

                # Chu·∫©n b·ªã ·∫£nh BGR ƒë·ªÉ v·∫Ω h√¨nh ch·ªØ nh·∫≠t
                if image_array.ndim == 3:
                    annotated_img = cv2.cvtColor(image_array, cv2.COLOR_RGB2BGR)
                else:
                    annotated_img = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)
                logger.info(f"[ANNOTATED] ·∫¢nh annotated shape: {annotated_img.shape}")

                for i, region in enumerate(regions):
                    try:
                        rx = int(region.get("x", 0))
                        ry = int(region.get("y", 0))
                        rw = int(region.get("width", 0))
                        rh = int(region.get("height", 0))
                        # V·∫Ω khung ƒë·ªè quanh v√πng nghi ng·ªù
                        cv2.rectangle(
                            annotated_img,
                            (rx, ry),
                            (rx + rw, ry + rh),
                            (0, 0, 255),
                            2,
                        )
                        logger.info(f"[ANNOTATED] ƒê√£ v·∫Ω region {i}: ({rx},{ry})-({rx+rw},{ry+rh})")
                    except Exception as e_region:
                        logger.warning(f"[ANNOTATED] L·ªói khi v·∫Ω region {i}: {str(e_region)}")

                success = cv2.imwrite(str(annotated_path), annotated_img)
                logger.info(f"[ANNOTATED] cv2.imwrite success={success}, path={annotated_path}")

                if success:
                    # URL t∆∞∆°ng ƒë·ªëi ƒë·ªÉ backend/nginx c√≥ th·ªÉ expose
                    annotated_image_url = f"/annotated-images/{analysis_id}.png"
                    logger.info(f"[ANNOTATED] ‚úÖ ƒê√£ sinh annotated image th√†nh c√¥ng: {annotated_image_url}")
                else:
                    logger.error(f"[ANNOTATED] ‚ùå cv2.imwrite th·∫•t b·∫°i cho {annotated_path}")
            else:
                logger.info("[ANNOTATED] Kh√¥ng c√≥ regions ƒë·ªÉ v·∫Ω")
        except Exception as e:
            logger.error(f"[ANNOTATED] ‚ùå Exception khi sinh annotated image cho analysis_id={analysis_id}: {str(e)}", exc_info=True)
            annotated_image_url = None

        # 10. Build response
        logger.info(f"[RESPONSE] Chu·∫©n b·ªã tr·∫£ v·ªÅ AnalysisResult:")
        logger.info(f"[RESPONSE]   - analysis_id: {analysis_id}")
        logger.info(f"[RESPONSE]   - predicted_class: {predicted_class}")
        logger.info(f"[RESPONSE]   - confidence: {confidence:.4f}")
        logger.info(f"[RESPONSE]   - heatmap_url: {heatmap_url}")
        logger.info(f"[RESPONSE]   - annotated_image_url: {annotated_image_url}")
        logger.info(f"[RESPONSE]   - has_positive_condition: {has_positive_condition}")
        
        return AnalysisResult(
            analysis_id=analysis_id,
            image_url=str(request.image_url),
            image_type=request.image_type,
            
            # Primary prediction
            predicted_class=predicted_class,
            confidence=confidence,
            
            # Risk assessment
            risk_level=risk_assessment.get('risk_level', 'Low'),
            risk_score=risk_assessment.get('risk_score', 0.0),
            
            # Eye condition results
            conditions=conditions,
            risk_assessment=risk_assessment,
            findings=findings,
            recommendations=recommendations,
            
            # AURA Core: Systemic health risks & vascular metrics
            systemic_health_risks=systemic_health_risks,
            vascular_metrics=vascular_metrics,
            
            # Annotations
            annotations=annotations,
            heatmap_url=heatmap_url,
            annotated_image_url=annotated_image_url,
            
            # Metadata
            processed_at=datetime.now(),
            model_version=model_service.model_version if model_service else "fallback",
            model_info=model_service.get_model_info() if model_service else {},
            processing_time_ms=0,  # Will be set by caller
            image_quality={
                'score': validation.get('quality_score', 0.0),
                'is_valid': validation.get('is_valid', False),
                'issues': validation.get('issues', []),
                'characteristics': validation.get('characteristics', {})
            }
        )
    finally:
        await processor.close()


def generate_fallback_results() -> Dict[str, Any]:
    """Generate fallback results when model is not available"""
    return {
        'predicted_class': 'NORMAL',
        'confidence': 0.5,
        'systemic_health_risks': {
            'cardiovascular': {
                'name': 'Cardiovascular Risk',
                'risk_score': 0.1,
                'risk_level': 'Minimal',
                'description': 'Risk of heart disease based on retinal vascular changes',
                'contributing_conditions': []
            },
            'diabetes': {
                'name': 'Diabetes Complications Risk',
                'risk_score': 0.1,
                'risk_level': 'Minimal',
                'description': 'Risk of diabetes-related complications',
                'contributing_conditions': []
            },
            'hypertension': {
                'name': 'Hypertension Risk',
                'risk_score': 0.1,
                'risk_level': 'Minimal',
                'description': 'Risk of high blood pressure based on vascular changes',
                'contributing_conditions': []
            },
            'stroke': {
                'name': 'Stroke Risk',
                'risk_score': 0.1,
                'risk_level': 'Minimal',
                'description': 'Risk of cerebrovascular events',
                'contributing_conditions': []
            }
        },
        'conditions': {
            'CNV': {
                'probability': 0.1,
                'positive': False,
                'threshold': 0.3,
                'severity': 'Not detected',
                'clinical_name': 'Choroidal Neovascularization',
                'icd10': 'H35.31',
                'description': 'Abnormal blood vessel growth beneath the retina',
                'confidence_interval': {'lower': 0.05, 'upper': 0.15}
            },
            'DME': {
                'probability': 0.1,
                'positive': False,
                'threshold': 0.3,
                'severity': 'Not detected',
                'clinical_name': 'Diabetic Macular Edema',
                'icd10': 'E11.311',
                'description': 'Fluid accumulation in the macula due to diabetes',
                'confidence_interval': {'lower': 0.05, 'upper': 0.15}
            },
            'DRUSEN': {
                'probability': 0.1,
                'positive': False,
                'threshold': 0.4,
                'severity': 'Not detected',
                'clinical_name': 'Drusen (Early AMD)',
                'icd10': 'H35.30',
                'description': 'Yellow deposits under the retina',
                'confidence_interval': {'lower': 0.05, 'upper': 0.15}
            },
            'NORMAL': {
                'probability': 0.7,
                'positive': True,
                'threshold': 0.5,
                'severity': 'Healthy',
                'clinical_name': 'Normal Retina',
                'icd10': None,
                'description': 'No significant abnormalities detected',
                'confidence_interval': {'lower': 0.65, 'upper': 0.75}
            }
        },
        'risk_assessment': {
            'risk_level': 'Low',
            'risk_score': 0.1,
            'primary_condition': 'NORMAL',
            'positive_conditions': [],
            'urgency': 'low',
            'requires_referral': False
        },
        'recommendations': [
            (
                "‚ö†Ô∏è Hi·ªán t·∫°i h·ªá th·ªëng ƒëang ch·∫°y ·ªü **ch·∫ø ƒë·ªô d·ª± ph√≤ng (m√¥ ph·ªèng)**, "
                "m√¥ h√¨nh AI th·ª±c ch∆∞a ƒë∆∞·ª£c t·∫£i ƒë·∫ßy ƒë·ªß. K·∫øt qu·∫£ d∆∞·ªõi ƒë√¢y **ch·ªâ mang t√≠nh tham kh·∫£o**, "
                "ƒë·ªô ch√≠nh x√°c c√≥ th·ªÉ th·∫•p h∆°n so v·ªõi c·∫•u h√¨nh chu·∫©n."
            ),
            (
                "Trong ch·∫ø ƒë·ªô d·ª± ph√≤ng, h·ªá th·ªëng **kh√¥ng ghi nh·∫≠n r√µ b·∫•t th∆∞·ªùng nghi√™m tr·ªçng n√†o tr√™n v√µng m·∫°c**, "
                "tuy nhi√™n ƒëi·ªÅu n√†y **kh√¥ng thay th·∫ø cho ƒë√°nh gi√° l√¢m s√†ng c·ªßa b√°c sƒ©**."
            ),
            (
                "üí° ƒê·ªÅ ngh·ªã b·∫°n t√°i kh√°m, ch·ª•p l·∫°i v√µng m·∫°c b·∫±ng h·ªá th·ªëng ƒë√£ c·∫•u h√¨nh ƒë·∫ßy ƒë·ªß m√¥ h√¨nh AI, "
                "ho·∫∑c thƒÉm kh√°m b√°c sƒ© chuy√™n khoa m·∫Øt trong v√≤ng **6‚Äì12 th√°ng**, "
                "ho·∫∑c s·ªõm h∆°n n·∫øu xu·∫•t hi·ªán b·∫•t k·ª≥ tri·ªáu ch·ª©ng b·∫•t th∆∞·ªùng n√†o v·ªÅ th·ªã l·ª±c."
            )
        ]
    }


def generate_findings_from_results(
    model_results: Dict[str, Any],
    validation: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """Generate clinical findings from model results (b·ªánh t·∫°i m·∫Øt + ch·∫•t l∆∞·ª£ng ·∫£nh)"""
    findings = []
    
    conditions = model_results.get('conditions', {})
    risk_assessment = model_results.get('risk_assessment', {})
    
    # Add findings for positive conditions
    for class_name, data in conditions.items():
        if class_name == 'NORMAL':
            continue
        
        if data.get('positive', False):
            prob = data.get('probability', 0.0)
            
            findings.append({
                "type": data.get('clinical_name', class_name),
                "severity": data.get('severity', 'Unknown'),
                "location": "Retina",
                "description": f"{data.get('clinical_name', class_name)} detected with {prob:.1%} probability",
                "probability": prob,
                "icd10": data.get('icd10'),
                "confidence_interval": data.get('confidence_interval', {}),
                "urgency": "high" if class_name in ['CNV', 'DME'] else "medium"
            })
    
    # If normal
    if not findings:
        normal_data = conditions.get('NORMAL', {})
        findings.append({
            "type": "Normal Retina",
            "severity": "Healthy",
            "location": "General",
            "description": f"Kh√¥ng ph√°t hi·ªán b·∫•t th∆∞·ªùng r√µ r·ªát tr√™n v√µng m·∫°c (ƒë·ªô tin c·∫≠y: {normal_data.get('probability', 0.5):.1%}).",
            "probability": normal_data.get('probability', 0.5),
            "urgency": "low"
        })
    
    # Add image quality note if needed
    quality_score = validation.get('quality_score', 1.0)
    if quality_score < 0.7:
        findings.append({
            "type": "Image Quality Note",
            "severity": "Warning",
            "location": "General",
            "description": f"Ch·∫•t l∆∞·ª£ng ·∫£nh ch∆∞a t·ªëi ∆∞u (ƒëi·ªÉm ch·∫•t l∆∞·ª£ng: {quality_score:.2f}). ƒê·ªô ch√≠nh x√°c ph√¢n t√≠ch c√≥ th·ªÉ b·ªã gi·∫£m, "
                           "n√™n c√¢n nh·∫Øc ch·ª•p l·∫°i ·∫£nh r√µ h∆°n n·∫øu ƒëi·ªÅu ki·ªán cho ph√©p.",
            "urgency": "low"
        })
    
    return findings


def generate_vascular_findings(vascular_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    T·∫°o th√™m c√°c finding li√™n quan ƒë·∫øn m·∫°ch m√°u v√µng m·∫°c
    ph·ª•c v·ª• ph·∫ßn 'B·∫•t th∆∞·ªùng M·∫°ch m√°u' tr√™n giao di·ªán.
    """
    findings: List[Dict[str, Any]] = []
    
    if not vascular_metrics:
        return findings
    
    tortuosity = float(vascular_metrics.get("tortuosity_index", 0.0))
    width_var = float(vascular_metrics.get("width_variation_index", 0.0))
    micro_count = int(vascular_metrics.get("microaneurysm_count", 0))
    hemorrhage = float(vascular_metrics.get("hemorrhage_score", 0.0))
    
    # ƒê·ªô xo·∫Øn m·∫°ch
    if tortuosity >= 0.6:
        findings.append({
            "type": "B·∫•t th∆∞·ªùng m·∫°ch m√°u - ƒê·ªô xo·∫Øn m·∫°ch",
            "severity": "Warning",
            "location": "Retina vessels",
            "description": "H·ªá th·ªëng ghi nh·∫≠n ƒë·ªô xo·∫Øn c·ªßa m·∫°ch m√°u v√µng m·∫°c ·ªü m·ª©c cao, "
                           "c·∫ßn l∆∞u √Ω t·∫ßm so√°t tƒÉng huy·∫øt √°p v√† b·ªánh l√Ω tim m·∫°ch.",
            "score": tortuosity,
            "urgency": "medium"
        })
    elif tortuosity >= 0.3:
        findings.append({
            "type": "M·∫°ch m√°u - ƒê·ªô xo·∫Øn trung b√¨nh",
            "severity": "Mild",
            "location": "Retina vessels",
            "description": "ƒê·ªô xo·∫Øn c·ªßa m·∫°ch m√°u v√µng m·∫°c tƒÉng nh·∫π, n√™n ƒë∆∞·ª£c theo d√µi ƒë·ªãnh k·ª≥ "
                           "k·∫øt h·ª£p v·ªõi ƒëo huy·∫øt √°p v√† ki·ªÉm tra tim m·∫°ch.",
            "score": tortuosity,
            "urgency": "low"
        })
    
    # Bi·∫øn thi√™n b·ªÅ r·ªông m·∫°ch
    if width_var >= 0.6:
        findings.append({
            "type": "B·∫•t th∆∞·ªùng ƒë∆∞·ªùng k√≠nh m·∫°ch m√°u",
            "severity": "Warning",
            "location": "Retina vessels",
            "description": "ƒê·ªô bi·∫øn thi√™n b·ªÅ r·ªông m·∫°ch m√°u cao, g·ª£i √Ω kh·∫£ nƒÉng c√≥ b·∫•t th∆∞·ªùng v·ªÅ √°p l·ª±c "
                           "ho·∫∑c th√†nh m·∫°ch, n√™n ƒë∆∞·ª£c ƒë√°nh gi√° th√™m.",
            "score": width_var,
            "urgency": "medium"
        })
    
    # Vi ph√¨nh m·∫°ch & xu·∫•t huy·∫øt
    if micro_count > 0 or hemorrhage >= 0.3:
        findings.append({
            "type": "B·∫•t th∆∞·ªùng m·∫°ch m√°u - Vi ph√¨nh / Xu·∫•t huy·∫øt nh·ªè",
            "severity": "Warning",
            "location": "Retina",
            "description": (
                "H·ªá th·ªëng ph√°t hi·ªán s·ªë l∆∞·ª£ng c·∫•u tr√∫c d·∫°ng ch·∫•m c√≥ th·ªÉ t∆∞∆°ng ·ª©ng vi ph√¨nh m·∫°ch "
                f"v√†/ho·∫∑c ƒëi·ªÉm xu·∫•t huy·∫øt nh·ªè (∆∞·ªõc l∆∞·ª£ng: {micro_count} v√πng nghi ng·ªù, ƒëi·ªÉm xu·∫•t huy·∫øt: {hemorrhage:.2f}). "
                "C·∫ßn ƒë∆∞·ª£c b√°c sƒ© chuy√™n khoa xem x√©t tr√™n l√¢m s√†ng v√† ·∫£nh g·ªëc."
            ),
            "score": max(min(hemorrhage, 1.0), 0.0),
            "urgency": "medium"
        })
    
    return findings


def enrich_systemic_risks_with_vascular(
    systemic_risks: Dict[str, Any],
    vascular_metrics: Dict[str, Any],
    base_risk_assessment: Dict[str, Any]
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    ƒêi·ªÅu ch·ªânh nguy c∆° s·ª©c kh·ªèe to√†n th√¢n d·ª±a th√™m tr√™n c√°c ch·ªâ s·ªë m·∫°ch m√°u v√µng m·∫°c.
    
    - Kh√¥ng thay ƒë·ªïi c·∫•u tr√∫c d·ªØ li·ªáu, ch·ªâ tinh ch·ªânh risk_score/risk_level.
    - D√πng c√°c ch·ªâ s·ªë: tortuosity_index, width_variation_index, hemorrhage_score.
    """
    if not systemic_risks or not vascular_metrics:
        return systemic_risks, base_risk_assessment
    
    tortuosity = float(vascular_metrics.get("tortuosity_index", 0.0))
    width_var = float(vascular_metrics.get("width_variation_index", 0.0))
    hemorrhage = float(vascular_metrics.get("hemorrhage_score", 0.0))
    
    # H·ªá s·ªë ·∫£nh h∆∞·ªüng c·ªßa ch·ªâ s·ªë m·∫°ch m√°u t·ªõi t·ª´ng nh√≥m nguy c∆°
    vascular_influence = {
        "cardiovascular": 0.25 * tortuosity + 0.25 * width_var + 0.25 * hemorrhage,
        "hypertension": 0.35 * tortuosity + 0.25 * width_var,
        "diabetes": 0.20 * hemorrhage,
        "stroke": 0.30 * hemorrhage + 0.20 * tortuosity,
    }
    
    updated_systemic = {}
    max_systemic_score = 0.0
    
    for key, data in systemic_risks.items():
        score = float(data.get("risk_score", 0.0))
        extra = float(vascular_influence.get(key, 0.0))
        
        # Gi·ªõi h·∫°n m·ª©c tƒÉng ƒë·ªÉ kh√¥ng l√†m m√©o logic g·ªëc
        boosted_score = min(1.0, score + min(extra, 0.25))
        
        # T√≠nh l·∫°i m·ª©c nguy c∆°
        if boosted_score >= 0.6:
            level = "High"
        elif boosted_score >= 0.3:
            level = "Moderate"
        elif boosted_score >= 0.1:
            level = "Low"
        else:
            level = "Minimal"
        
        updated_entry = dict(data)
        updated_entry["risk_score"] = boosted_score
        updated_entry["risk_level"] = level
        updated_systemic[key] = updated_entry
        
        max_systemic_score = max(max_systemic_score, boosted_score)
    
    # C·∫≠p nh·∫≠t t·ªïng quan risk_assessment (k·∫øt h·ª£p ƒëi·ªÅu ch·ªânh t·ª´ vascular metrics)
    risk_assessment = dict(base_risk_assessment or {})
    current_level = risk_assessment.get("risk_level", "Minimal")
    base_score = float(risk_assessment.get("risk_score", 0.0))
    
    # ƒêi·ªÉm k·∫øt h·ª£p m·ªõi d·ª±a tr√™n max_systemic_score
    combined_score = max(base_score, max_systemic_score * 0.8)
    
    # N√¢ng m·ª©c n·∫øu m·∫°ch m√°u r·∫•t x·∫•u d√π ban ƒë·∫ßu ƒë√°nh gi√° th·∫•p
    max_vascular = max(tortuosity, width_var, hemorrhage)
    if max_vascular >= 0.7 and current_level in ["Minimal", "Low"]:
        new_level = "Medium"
    else:
        # √Ånh x·∫° l·∫°i theo combined_score
        if combined_score >= 0.7:
            new_level = "High"
        elif combined_score >= 0.4:
            new_level = "Medium"
        elif combined_score >= 0.3:
            new_level = "Low"
        else:
            new_level = "Minimal"
    
    risk_assessment["risk_level"] = new_level
    risk_assessment["combined_risk_score"] = float(combined_score)
    risk_assessment["risk_score"] = float(base_score)
    
    return updated_systemic, risk_assessment


def generate_annotations(features: Dict[str, Any], image_shape: Tuple[int, ...]) -> Dict[str, Any]:
    """Generate annotations for the image (v√πng nghi ng·ªù d·ª±a tr√™n s·ªë b·∫•t th∆∞·ªùng)"""
    import random
    
    height, width = image_shape[:2]
    
    abnormal_regions: List[Dict[str, Any]] = []
    abnormalities_count = int(features.get('potential_abnormalities_count', 0))
    vascular_metrics = features.get('vascular_metrics', {})
    hemorrhage_score = float(vascular_metrics.get('hemorrhage_score', 0.0)) if vascular_metrics else 0.0
    
    # S·ªë v√πng ƒë√°nh d·∫•u ph·ª• thu·ªôc c·∫£ s·ªë b·∫•t th∆∞·ªùng l·∫´n ƒëi·ªÉm xu·∫•t huy·∫øt
    base_regions = min(abnormalities_count, 5)
    extra_from_hemorrhage = 1 if hemorrhage_score >= 0.4 else 0
    regions_to_draw = max(0, min(6, base_regions + extra_from_hemorrhage))
    
    random.seed(42)
    
    for i in range(regions_to_draw):
        x = random.randint(50, max(51, width - 100))
        y = random.randint(50, max(51, height - 100))
        w = random.randint(30, 80)
        h = random.randint(30, 80)
        
        abnormal_regions.append({
            "x": x,
            "y": y,
            "width": w,
            "height": h,
            "type": "potential_abnormality",
            "confidence": float(max(0.4, min(0.9, 0.5 + hemorrhage_score / 2.0 + random.uniform(-0.05, 0.05))))
        })
    
    return {
        "abnormal_regions": abnormal_regions,
        "image_dimensions": {
            "width": int(width),
            "height": int(height)
        }
    }

# ============================================================================
# Model Management Endpoints
# ============================================================================

@app.post("/api/models/load")
async def load_model_endpoint(
    model_path: Optional[str] = Query(None, description="Local path to model file"),
    model_version: str = Query("v2.0.0", description="Model version string")
):
    """
    Load AI model into memory
    
    By default, loads the tomalmog/oct-retinal-classifier from Hugging Face.
    """
    global model_loaded, model_service
    
    try:
        if model_service is None:
            try:
                from app.model_service import RetinalModelService
                model_service = RetinalModelService()
            except ImportError:
                raise HTTPException(
                    status_code=503,
                    detail="Model service not available"
                )
        
        logger.info(f"Loading model version: {model_version}")
        
        success = model_service.load_model(model_path, model_version)
        
        if success:
            model_loaded = model_service.model_loaded
            return {
                "status": "success",
                "message": f"Model loaded successfully",
                "model_version": model_version,
                "model_info": model_service.get_model_info()
            }
        else:
            raise HTTPException(
                status_code=500,
                detail="Failed to load model"
            )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error loading model: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to load model: {str(e)}")

@app.get("/api/models/status")
async def model_status():
    """Get current model status and information"""
    if model_service is None:
        return {
            "model_loaded": False,
            "error": "Model service not initialized"
        }
    
    info = model_service.get_model_info()
    return {
        "model_loaded": info.get('model_loaded', False),
        "model_version": info.get('model_version'),
        "model_id": info.get('model_id'),
        "architecture": info.get('architecture'),
        "use_mock": info.get('use_mock', True),
        "pytorch_available": info.get('pytorch_available', False),
        "device": info.get('device', 'cpu'),
        "gpu_available": info.get('gpu_available', False),
        "classes": info.get('classes', []),
        "num_classes": info.get('num_classes', 0)
    }

@app.get("/api/models/classes")
async def get_model_classes():
    """Get list of classes the model can detect"""
    if model_service is None:
        return {
            "classes": ['CNV', 'DME', 'DRUSEN', 'NORMAL'],
            "descriptions": {
                'CNV': 'Choroidal Neovascularization - Abnormal blood vessel growth',
                'DME': 'Diabetic Macular Edema - Fluid accumulation due to diabetes',
                'DRUSEN': 'Early AMD - Yellow deposits under the retina',
                'NORMAL': 'Healthy Retina - No significant abnormalities'
            }
        }
    
    return {
        "classes": model_service.OCT_CLASSES,
        "condition_mapping": model_service.CONDITION_MAPPING
    }


@app.get("/api/systemic-health-risks")
async def get_systemic_health_risks():
    """
    Get list of systemic health risks that AURA can detect
    
    AURA uses retinal imaging as a "window" to systemic health,
    detecting risks of cardiovascular disease, diabetes complications,
    hypertension, and stroke based on retinal vascular changes.
    """
    if model_service is None:
        return {
            "systemic_risks": ['cardiovascular', 'diabetes', 'hypertension', 'stroke'],
            "description": "AURA detects systemic health risks from retinal findings"
        }
    
    return {
        "systemic_risks": list(model_service.SYSTEMIC_HEALTH_RISKS.keys()),
        "risk_details": model_service.SYSTEMIC_HEALTH_RISKS,
        "description": "AURA uses retinal imaging as a 'window' to systemic health"
    }

# ============================================================================
# Root Endpoint
# ============================================================================

@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "service": "AURA AI Core Microservice",
        "description": "Comprehensive AI Understanding Retinal Analysis - Using retinal imaging as a window to systemic health",
        "version": "2.0.0",
        "status": "running",
        "model": {
            "name": "OCT Retinal Classifier",
            "source": "tomalmog/oct-retinal-classifier",
            "architecture": "EfficientNet-B3",
            "accuracy": "99.6%",
            "eye_conditions": ["CNV", "DME", "DRUSEN", "NORMAL"]
        },
        "aura_features": {
            "systemic_health_detection": True,
            "detectable_risks": ["cardiovascular", "diabetes", "hypertension", "stroke"],
            "description": "Estimates systemic health risks from retinal vascular analysis"
        },
        "endpoints": {
            "health": "/health",
            "analyze": "/api/analyze",
            "model_status": "/api/models/status",
            "model_classes": "/api/models/classes",
            "systemic_risks": "/api/systemic-health-risks",
            "docs": "/docs"
        }
    }

# ============================================================================
# Startup/Shutdown Events
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """Initialize service on startup"""
    global model_loaded, model_service
    
    logger.info("=" * 60)
    logger.info("AURA AI Core Microservice starting up...")
    logger.info("Model: tomalmog/oct-retinal-classifier (EfficientNet-B3)")
    logger.info("=" * 60)
    
    # Initialize model service
    try:
        from app.model_service import RetinalModelService
        model_service = RetinalModelService()
        
        # Load model
        model_version = os.getenv("MODEL_VERSION", "v2.0.0")
        model_path = os.getenv("MODEL_PATH", None)
        
        logger.info(f"Loading model version: {model_version}")
        success = model_service.load_model(model_path, model_version)
        
        if success:
            model_loaded = model_service.model_loaded
            info = model_service.get_model_info()
            logger.info(f"Model loaded successfully!")
            logger.info(f"  - Model ID: {info.get('model_id')}")
            logger.info(f"  - Architecture: {info.get('architecture')}")
            logger.info(f"  - Device: {info.get('device')}")
            logger.info(f"  - GPU Available: {info.get('gpu_available')}")
            logger.info(f"  - Classes: {info.get('classes')}")
        else:
            logger.warning("Model loaded with fallback (mock predictions)")
            
    except ImportError as e:
        logger.error(f"Failed to import model service: {str(e)}")
        logger.warning("AI Core will use mock predictions")
    except Exception as e:
        logger.error(f"Error initializing model service: {str(e)}")
        logger.warning("AI Core will use mock predictions")
    
    logger.info("=" * 60)
    logger.info("AURA AI Core Microservice started successfully!")
    logger.info("Docs available at: /docs")
    logger.info("=" * 60)

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("AURA AI Core Microservice shutting down...")

# ============================================================================
# Main Entry Point
# ============================================================================

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=False,
        log_level="info"
    )
